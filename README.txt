************************
Mitch Mathieu          *
10157108               *
CISC 452 Assignment 2  *
October 11 2019        *
************************

-- ANN Design --

Epochs: 3
    I chose 3 epochs as it gave the network more than one pass at the training data, but kept 
    the overall training time reasonable. Ideally the network would train for longer - increasing
    the epochs is an improvement I would make to improve performance.

Learning Rate: 0.5
    In typical fashion, we don't want the learning rate to be too large and make drastic changes
    to the weights, but we don't want it to be so small that training is overly slow. 0.5 seems
    like a good compromise.

Momentum: 0.5
    The momentum value was determined using the same rationale as the learning rate. Too large
    and the network won't react quick enough. Too small and it will get stuck at local minima.

Hidden Layers: 1 layer with 10 neurons
    I chose to use a single hidden layer mostly to keep the training time reasonable. I added
    10 neurons to increase the ability of the network to distinguish unique features between
    the numbers, without adding too much complexity to the overall network.

Output Neurons: 10
    10 output neurons was the clear choice as there are 10 possible labels for the data. This
    way each output neuron only needs to classify one label.


-- Accuracy and Performance --

    Regrettably, my network predicts the same class for every single set of inputs. This
    gives it an accuracy of about %0.0001. Like a broken clock, it sticks to one answer
    and happens to get it right every once in a while.

    The predictions from the network don't seem to change from adding more nodes or epochs
    which makes me think it's a bug in the code. The actual algorithms for forward and
    backward propagation, as well as weight adjustment, seem to be sound but unfortuantely
    I ran out of time to debug further.

    To see the precision, recall and confusion matrix, see 'output.txt' generated by the code.
    (honestly, it's worth it to see such a silly looking confusion matrix).